---
layout: default
---

[Call for Papers](#call-for-papers) |
[Submission Guidelines](#submission-guidelines) |
[Important Dates](#important-dates) |
[Organizers](#organizers)

## About the Workshop

LLMIT (pronounced as "limit") will be a full day workshop at [CIKM 2023](https://uobevents.eventsair.com/cikm2023/).
 
### Background

Large language models (LLMs), when scaled from millions to billions of parameters, have been demonstrated to exhibit the so-called 'emergence' effect, in that they are not only able to produce semantically correct and coherent text, but are also able to adapt themselves surprisingly well with small changes in contexts supplied as inputs (commonly called prompts).
Despite producing semantically coherent and potentially relevant text for a given context, LLMs are vulnerable to yield incorrect information. This misinformation generation, or the so-called hallucination problem of an LLM, gets worse when an adversary manipulates the prompts to their own advantage, e.g., generating false propaganda to disrupt communal harmony, generating false information to trap consumers with target consumables etc. Not only does the consumption of an LLM-generated hallucinated content by humans pose societal threats, such misinformation, when used as prompts, may lead to detrimental effects for in-context learning (also known as few-shot prompt learning).

### Scope

With reference to the above-mentioned problems of LLM usage, in this workshop, **Large Language Models’ Interpretability and Trustworthiness (LLMIT)**, we intend to foster research on topics related to not only identifying misinformation from LLM-generated content, but also to mitigate the propagation effects of this generated misinformation on downstream predictive tasks thus leading to more robust and effective leveraging in-context learning. It is critical that researchers work towards a responsible and trustworthy innovation. For instance, identifying the source documents which the decoding phase of an LLM largely utilised to generate an output text can potentially help a user better understand the content, thus possibly contributing towards developing a trust towards the model. Relevant active areas in LLMs include misinformation detection, in-context learning, detection and mitigation of biases using prompts, alleviating adversarial attacks etc. for several downstream tasks to aid the goal of responsible AI. On the other hand, these responsible AI objectives are often viewed independently to each other, which is a significant constraint because it is necessary to comprehend the interplay and/or conflict between them. The purpose of this workshop is to bring together researchers working on those distinct yet connected areas, as well as their overlap, in order to move towards a more thorough understanding of trustworthy use of LLMs.


## Call for Papers

We solicit submissions on (but not limited to) the following research topics.

1. **Misinformation detection**: It is a widely known fact that the text generated by LLMs are often of hallucinatory in nature. Consequently, mapping back to the original documents (attribution at document level) which were likely responsible in affecting the decoding path to generate the text for a downstream task can throw insights on an LLM’s 0-shot or few-shot (in-context) task-specific abstraction of the input data.
2. **Prompt or In-Context Explanations** of the LLM-generated content in terms of attention-based or counter-factual explanations of the prompts, i.e., which parts of the prompts are more important in determining the class probabilities (via generation of class-specific sets of words by the decoder).
3. **Linking LLM-generated answers to knowledge bases** or use knowledge bases to formulate template-driven prompts.
4. **LLMs for generating weak labels** for various applications, or to generate simulated data (silver-standard ground-truth) to reduce annotation effort.
5. **Adaptive In-context learning for LLMs**, i.e., work towards developing a transparent LLM-based in-context learning model that explains the different choices employed in prompt learning, which may include -
   - What similarity metric to employ for generating the few-shot examples, e.g., sparse vectors, dense embedding, task-specific dense embedding etc.
   - How many few shot examples to use.
   - Explore combination (apriori and post-hoc) of LLM-based in-context learning with supervised parametric models.
6. **Fair Predictions with LLMs**, i.e., mitigate the detrimental effects of biased responses with suitable prompts.
7. **Adversarial Robustness of LLMs**, i.e., optimise the robustness of LLM-based in context learning to adversarial attacks based on prompt injections.
8. **LLM-driven in-context learning for search and recommendation**, i.e., explore the potential of LLMs for personalised search and recommendation. Since LLMs have been shown to work well with small quantities of task-specific training data, they can potentially be used to improve the effectiveness of personalized search and recommendation.
9. **Multi-modal LLMs**, involving exploration on the topics related to the interpretability and trustworthiness of LLMs in the particular context of multi-modal predictive tasks, e.g., visual question answering etc.
10. **Ethical concerns of LLMs**, i.e., research on topics related to a responsible use of LLMs and their socio-economic implications.


## Submission Guidelines

Papers submitted should be **4 to 10 pages of content** (plus any number of additional pages for references). Papers must be submitted in PDF as per the [CEUR](https://www.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/wqyfdgftmcfw) single-column conference format. The review process is **double-blind**. Papers should be uploaded via Easychair via the following submission link.

- Submit your papers via this [link](https://easychair.org/conferences/?conf=llmit23).

Accepted papers will be included in the CIKM'23 companion volume (**published by CEUR** and **indexed by DBLP**). At least one author of each accepted contribution must attend the workshop. No additional authors can be added after acceptance.

## Important Dates

The following dates are in AoE (Anywhere on Earth) timezone.

* Paper Submission Deadline - ~~30th August~~ 4th September, 2023
* Review Notifications - ~~18th September~~ 25th September, 2023
* Camera-ready Due - ~~25th September~~ 30th September, 2023
* Workshop Day - 22nd October, 2023

## Organizers

* [Tulika Saha](https://sahatulika15.github.io/), University of Liverpool, United Kingdom
* [Debasis Ganguly](https://gdebasis.github.io/), University of Glasgow, United Kingdom
* [Sriparna Saha](https://www.iitp.ac.in/~sriparna/), Indian Institute of Technology Patna, India
* [Prasenjit Mitra](https://ist.psu.edu/directory/pum10), Pennsylvania State University, USA

For any questions, send an email to this [alias](mailto:llmit23@easychair.org).

Please reach out to the organizers for any questions.
